# -*- coding: utf-8 -*-
"""Submission2_Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ld41GE4wDakk4qRpJzdZkpbpaWeqigL
"""

# baca data
import pandas as pd
df = pd.read_csv('https://query.data.world/s/tu7xkafdc4jbuzmuh5zxqaeshwj7sv', encoding= 'unicode_escape')
df.head()

# hapus data yang tidak diperlukan
df.drop(['AverageTemperatureUncertainty', 'City','Latitude','Longitude'], axis=1, inplace=True)
df

df['Country'].unique()

import numpy as np

# ambil data 1 negara
df = df.loc[df['Country'].isin(['Indonesia'])]
df['dt'] = pd.to_datetime((df['dt']).apply(str))
df.AverageTemperature.replace(-99, np.NaN, inplace=True)
df.AverageTemperature.ffill(inplace=True)
df = df.loc[(df.dt >= '1900-01-01') & (df.dt < '2013-09-01'), ['dt', 'AverageTemperature']]
df.set_index('dt', inplace= True)
df.head()

df.isnull().sum()

from sklearn.model_selection import train_test_split
train, test = train_test_split(df.values, test_size=0.2, shuffle=False)

# Rescale data
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
train_scale = scaler.fit_transform(train.reshape(-1, 1))
test_scale = scaler.fit_transform(test.reshape(-1, 1))

split=int((1-0.2)*len(df))

df_train = df.index[:split]
df_test = df.index[split:]

from keras.preprocessing.sequence import TimeseriesGenerator
look_back = 20
train_gen = TimeseriesGenerator(train_scale, train_scale, length=look_back, batch_size=20)     
test_gen = TimeseriesGenerator(test_scale, test_scale, length=look_back, batch_size=1)

import tensorflow as tf
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(32, activation='relu', return_sequences=True, input_shape=(look_back, 1)),
  tf.keras.layers.GlobalMaxPooling1D(),
  tf.keras.layers.Dropout(0.25),
  tf.keras.layers.Dense(1)
])

MAE = (df['AverageTemperature'].max() - df['AverageTemperature'].min()) * 10/100
print("MAE 10%  = " + str(MAE))

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<0.15 and logs.get('val_mae')<0.15):
      print("\nMAE dari model < 10% skala data")
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(
    loss=tf.keras.losses.Huber(),
    optimizer=optimizer,
    metrics=["mae"]
)

history = model.fit(
    train_gen,
    validation_data = test_gen,
    epochs=10,
    verbose=1,
    callbacks=[callbacks]
)

# evaluasi performa model yang telah dibuat
import matplotlib.pyplot as plt

def eval_plot(history):

  plt.figure(figsize=(14, 5))

  # Accuracy plot
  plt.subplot(1, 2, 1)
  acc = history.history['mae']
  val_acc = history.history['val_mae']
  epochs = range(len(acc))
  acc_plot, = plt.plot(epochs, acc, 'r')
  val_acc_plot, = plt.plot(epochs, val_acc, 'b')
  plt.title('Training and Validation Accuracy')
  plt.legend([acc_plot, val_acc_plot], ['Training Accuracy', 'Validation Accuracy'])

  # Loss plot
  plt.subplot(1, 2, 2)
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(len(loss))
  loss_plot, = plt.plot(epochs, loss, 'r')
  val_loss_plot, = plt.plot(epochs, val_loss, 'b')
  plt.title('Training and Validation Loss')
  plt.legend([loss_plot, val_loss_plot], ['Training Loss', 'Validation Loss'])

eval_plot(history)
# -*- coding: utf-8 -*-
"""Submission1-BBC-News-Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vRRrgTChpq_2eCx6_ximOe9t6k8wFw5x
"""

import pandas as pd

df = pd.read_csv('bbc-text.csv', on_bad_lines='skip')
df.head()

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords 
from textblob import Word

STOPWORDS = stopwords.words("english")

def clean(text):
    text = text.lower()
    text = re.sub("[^\w\s]","",text) # Remove punctuations 
    text = " ".join(w for w in text.split() if w not in STOPWORDS)
    text = " ".join([Word(word).lemmatize() for word in text.split()])
    return text

def remove_duplicates(text):
    pattern = re.compile(r"(.)\1{2,}")
    return pattern.sub(r"\1\1", text)

df['text'] = df['text'].apply(lambda x : clean(x)) 
df['text'] = df['text'].apply(lambda x :  remove_duplicates(x))

df['category'].unique()

# one-hot-endcoding
category = pd.get_dummies(df.category)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='category')
new_df

# Ubah tipe dataframe ke array
text = new_df['text'].values
label = new_df[['tech', 'business', 'sport', 'entertainment', 'politics']].values

# bagi data menjadi train dan test
from sklearn.model_selection import train_test_split
text_train, text_test, label_train, label_test = train_test_split(text, label, test_size=0.2)

# tokenisasi
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='-')
tokenizer.fit_on_texts(text_train) 

print(tokenizer.word_index)

sequences_train = tokenizer.texts_to_sequences(text_train)
sequences_test = tokenizer.texts_to_sequences(text_test)

# padded_train = pad_sequences(sequences_train)
# padded_test = pad_sequences(sequences_test)
 
padded_train = pad_sequences(sequences_train,
    padding='post',
    maxlen=100,
    truncating='post'
) 
padded_test = pad_sequences(
    sequences_test,
     padding='post',
    maxlen=100,
    truncating='post'
)

# Buat model
import tensorflow as tf
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax'),
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.95):
      print("\nAkurasi telah mencapai >95%!")
      self.model.stop_training = True
callbacks = myCallback()

history = model.fit(
    padded_train,
    label_train,
    validation_data=(padded_test, label_test),
    epochs=30,
    verbose=2,
    callbacks=[callbacks]
)

import matplotlib.pyplot as plt

# evaluasi performa model yang telah dibuat
def eval_plot(history):

  plt.figure(figsize=(14, 5))

  # Accuracy plot
  plt.subplot(1, 2, 1)
  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']
  epochs = range(len(acc))
  acc_plot, = plt.plot(epochs, acc, 'r')
  val_acc_plot, = plt.plot(epochs, val_acc, 'b')
  plt.title('Training and Validation Accuracy')
  plt.legend([acc_plot, val_acc_plot], ['Training Accuracy', 'Validation Accuracy'])

  # Loss plot
  plt.subplot(1, 2, 2)
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(len(loss))
  loss_plot, = plt.plot(epochs, loss, 'r')
  val_loss_plot, = plt.plot(epochs, val_loss, 'b')
  plt.title('Training and Validation Loss')
  plt.legend([loss_plot, val_loss_plot], ['Training Loss', 'Validation Loss'])

eval_plot(history)